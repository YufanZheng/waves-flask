@prefix rdf:    <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dct:    <http://purl.org/dc/terms/> .
@prefix foaf:   <http://xmlns.com/foaf/0.1/> .
@prefix xsd:    <http://www.w3.org/2001/XMLSchema#> .
@prefix waves:  <http://www.waves-rsp.org/configuration#> .
@prefix rdfs:	<http://www.w3.org/2000/01/rdf-schema#> .
@base <http://localhost:9091/waves/versailles/> .

<http://localhost:9091/waves/first-tests> {

  <installation>
	rdf:type        	waves:Installation ;
	rdfs:label 		"installation" ;
	waves:name		"first-tests" ;

	# ZooKeeper configuration (not to use Storm default ZooKeeper server)
	# Note: This property is required when using Storm Kafka spout that
	#       discovers Kafka servers through their ZooKeeper registration.
	waves:zookeeperHosts	"172.17.0.2:2181" ;

	# Global Redis database
	waves:redisHost	"172.17.0.6:6379" ;

	# Waves shifted and accelerated system clock configuration.
	waves:clock	[
		# For shifted clock, a start date is mandated.
		#waves:startDate	"2015-10-03T11:24:00Z"^^xsd:dateTime ;
		# Acceleration factor can be defined as an interval,
		# with a start date (see above), an end date:
		#waves:endDate		"2015-10-11T09:38:00Z"^^xsd:dateTime ;
		# and an actual (real time) duration for the interval:
		#waves.duration		"PT24H"^^xsd:duration ;
		# Or directly as a double value:
		waves:acceleration	"1.0"^^xsd:double
	] ;
	# Waves metrics collection.
	waves:metrics	[
		# Report metrics every 15 seconds.
		waves:frequency		"PT15S"^^xsd:duration ;
		# Report metrics to local console only.
		waves:reporters	"console"
		# waves:reporters	"influxdb, jvm"
	] .

  <stream/1i>
	rdf:type		waves:Stream ;
	rdfs:label 		"S-1i" ;
	waves:installation	<installation> ;
	waves:id		"54"^^xsd:integer .

  <stream/3f>
	rdf:type		waves:Stream ;
	rdfs:label 		"S-3f" ;
	waves:installation	<installation> ;
	waves:id		"123"^^xsd:integer .

  <filter/3f>
	rdf:type		waves:Filter ;
	rdfs:label		"F-3f" ;
	waves:installation	<installation> ;
	waves:consumesStream	_:F-3f_S-1i ;
	waves:producesStream	<stream/3f> ;
	#waves:rdfStore		_:b1 ;
	#waves:staticFeed	_:b2, _:b3 ;
	waves:stepRate		"PT3S"^^xsd:duration ;
	waves:query		"""
		PREFIX w: <http://www.waves-rsp.org/>
		PREFIX dct: <http://purl.org/dc/terms/>
		PREFIX wgs84: <http://www.w3.org/2003/01/geo/wgs84_pos#>
		PREFIX sens-obs: <http://knoesis.wright.edu/ssw/>
		CONSTRUCT {
			?eventId dct:issued ?step ; w:observation ?obs .
		} WHERE {
			?obs a wgs84:Point .
		}""" ;
	waves:numTasks		"3"^^xsd:int ;
	waves:workers		"2"^^xsd:int .

  _:F-3f_S-1i
	rdfs:label		"F-3f_S-1i" ;
	waves:stream		<stream/1i> ;
	waves:windowSpan	"PT8S"^^xsd:duration .

  # Datalift-based RDF back-end store
  _:b1	rdf:type		waves:RdfStore ;
	rdfs:label		"datalift-waves" ;
	waves:location		"http://localhost:9091/openrdf-sesame/repositories/waves" ;
	#waves:login		"usr1" ;
	#waves:password		"pwd1" ;
	waves:storeType		"sesame" .

  # Static data SPARQL feed
  _:b2	rdf:type		waves:SparqlFeed ;
	rdfs:label 		"sparql1Feed" ;
	waves:feedType 		"SPARQL" ;
	waves:location		"http://localhost:9091/sparql" ;
	waves:refreshInterval	"PT2M"^^xsd:duration ;
	waves:query		"""
		CONSTRUCT { ?s ?p ?o . }
		WHERE { GRAPH <http://localhost:9091/kiosques> { ?s ?p ?o . } }""" .

  # Static data RDF document feed
  _:b3	rdf:type		waves:RdfFeed ;
	rdfs:label 		"doc1Feed" ;
	waves:feedType 		"RdfDocument" ;
	waves:location		"file:///tmp/sample1.ttl" .

  # Test event source for stream #54 (a.k.a. 0x1i)
  # This source needs to be defined only to generate events from tests data
  # files. If no such source is defined, Waves will read events from Kafka.
  <source/1i>
	# RDF Spout: Use the following label to use file-based RDF spout
	# in test topology.
	rdfs:label 		"QIn-3f_S-1i" ;
	# Kafka RDF producer: Use the following label to run the file-based RDF
	# Kafka producer to inject test events into the Kafka topic.
	#rdfs:label 		"S-1i.kafka" ;
	waves:installation	<installation> ;
	waves:inputFolder	"/usr/local/srcFolder" ;
	waves:inputPattern	".n3$" ;
	waves:eventRate		"PT2S"^^xsd:duration ;
	waves:numTasks		"4"^^xsd:int .

  # Test event sink for stream #123 (a.k.a. 0x3f)
  # This sink needs to be defined only to log produced Waves events. If no
  # such sink is defined, Waves will output events into Kafka.
  <sink/3f>
	# RDF Outputter bolt: Use the following label to use file-based RDF
	# outputter in test topology.
	rdfs:label 		"QOut-3f" ;
	# Kafka RDF consumer: Use the following label to use file-based RDF
	# Kafka consumer to consume the Kafka messages output by the test
	# topology and dump them as RDF text.
	#rdfs:label 		"S-3f.kafka" ;
	waves:installation	<installation> ;
	# Use the following 2 properties to output to file or stream.
	waves:outputFile	"stdout" ;
	waves:outputFormat	"Turtle" ;
	# Use the following property to output to an RDF store.
	# waves:rdfStore	_:b1 ;
	waves:numTasks		"2"^^xsd:int .

}
